# 深度学习


## 前向后向传播

## Norm
- BN用来减少 “Internal Covariate Shift” 来加速网络的训练
- BN 和 ResNet 的作用类似，都使得 loss landscape 变得更加光滑了 (How Does Batch Normalization Help Optimization)
- BN在训练和测试过程中，其均值和方差的计算方式是不同的。测试过程中采用的是基于训练时估计的统计值，训练过程中则是采用指数加权平均计算
- BN,当 batch 较小时不具备统计意义，而加大的 batch 又收到硬件的影响；BN 适用于 DNN、CNN 之类固定深度的神经网络，而对于 RNN 这类 sequence 长度不一致的神经网络来说，会出现 sequence 长度不同的情况

## CNN

```python
# https://github.com/openai/gpt-2/blob/master/src/model.py
def conv1d(x, scope, nf, *, w_init_stdev=0.02):
    with tf.variable_scope(scope):
        *start, nx = shape_list(x)
        w = tf.get_variable('w', [1, nx, nf], initializer=tf.random_normal_initializer(stddev=w_init_stdev))
        b = tf.get_variable('b', [nf], initializer=tf.constant_initializer(0))
        c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, start+[nf])
        return c
```

## 优化
https://blog.csdn.net/S20144144/article/details/103417502

SGD原理



### 学习率scheduler
- https://www.kaggle.com/code/isbhargav/guide-to-pytorch-learning-rate-scheduling


## 损失函数
- https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-pytorch-loss-functions.md

cross entropy 
- https://gombru.github.io/2018/05/23/cross_entropy_loss/
binary cross entropy


对数损失
- https://www.zhihu.com/question/27126057


## 网络

### word2vec/glove/fasttext

### RNN/LSTM/GRU
公式

梯度爆炸与梯度消失

### Transformer

- attention

### BERT

### GPT

### YOLO

### wide&deep
