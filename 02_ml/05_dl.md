# 深度学习


## 前向后向传播

## Norm
- BN用来减少 “Internal Covariate Shift” 来加速网络的训练
- BN 和 ResNet 的作用类似，都使得 loss landscape 变得更加光滑了 (How Does Batch Normalization Help Optimization)
- BN在训练和测试过程中，其均值和方差的计算方式是不同的。测试过程中采用的是基于训练时估计的统计值，训练过程中则是采用指数加权平均计算
- BN,当 batch 较小时不具备统计意义，而加大的 batch 又收到硬件的影响；BN 适用于 DNN、CNN 之类固定深度的神经网络，而对于 RNN 这类 sequence 长度不一致的神经网络来说，会出现 sequence 长度不同的情况

## CNN

```python
# https://github.com/openai/gpt-2/blob/master/src/model.py
def conv1d(x, scope, nf, *, w_init_stdev=0.02):
    with tf.variable_scope(scope):
        *start, nx = shape_list(x)
        w = tf.get_variable('w', [1, nx, nf], initializer=tf.random_normal_initializer(stddev=w_init_stdev))
        b = tf.get_variable('b', [nf], initializer=tf.constant_initializer(0))
        c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, start+[nf])
        return c
```

## 优化
https://blog.csdn.net/S20144144/article/details/103417502

SGD原理

Adam和adgrad区别和应用场景


### 学习率scheduler
- https://www.kaggle.com/code/isbhargav/guide-to-pytorch-learning-rate-scheduling


## 损失函数
- https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-pytorch-loss-functions.md

cross entropy 
- https://gombru.github.io/2018/05/23/cross_entropy_loss/
binary cross entropy


对数损失
- https://www.zhihu.com/question/27126057


## 网络

### word2vec/glove/fasttext

### RNN/LSTM/GRU
公式

- 梯度爆炸与梯度消失
- 长距离依赖问题


### Transformer

- attention
  - 通过使用key向量，模型可以学习到不同模块之间的相似性和差异性，即对于不同的query向量，它可以通过计算query向量与key向量之间的相似度，来确定哪些key向量与该query向量最相似。

- attention为什么除以根号下d
  - 如果输入向量的维度d比较大，那么内积的结果也可能非常大，这会导致注意力分数也变得非常大，这可能会使得softmax函数的计算变得不稳定，并且会影响模型的训练和推理效果。通过除以根号d，可以将注意力分数缩小到一个合适的范围内，从而使softmax函数计算更加稳定，并且更容易收敛。

- layer norm的作用
  - LayerNorm可以对输入进行归一化，使得每个神经元的输入具有相似的分布特征，从而有助于网络的训练和泛化性能。此外，由于归一化的系数是可学习的，网络可以根据输入数据的特点自适应地学习到合适的归一化系数。
  - 加速模型的训练。由于输入已经被归一化，不同特征之间的尺度差异较小，因此优化过程更容易收敛，加快了模型的训练速度。
  
- warmup预热学习率
  - 在训练开始时，模型的参数初始值是随机的，模型还没有学到有效的特征表示。如果此时直接使用较大的学习率进行训练，可能会导致模型的参数值更新过快，从而影响模型的稳定性和收敛速度。此时使用warmup预热学习率的策略可以逐渐增加学习率，使得模型参数逐渐收敛到一定的范围内，提高模型的稳定性和收敛速度。

- 使用正弦作为位置编码
  - 提供位置信息


### BERT

### GPT

### YOLO

### wide&deep


## reference
[小白都能看懂的超详细Attention机制详解 - 雅正冲蛋的文章 - 知乎](https://zhuanlan.zhihu.com/p/380892265)
