# 深度学习

## 前向后向传播

## CNN

```python
# https://github.com/openai/gpt-2/blob/master/src/model.py
def conv1d(x, scope, nf, *, w_init_stdev=0.02):
    with tf.variable_scope(scope):
        *start, nx = shape_list(x)
        w = tf.get_variable('w', [1, nx, nf], initializer=tf.random_normal_initializer(stddev=w_init_stdev))
        b = tf.get_variable('b', [nf], initializer=tf.constant_initializer(0))
        c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, start+[nf])
        return c
```

## 优化
https://blog.csdn.net/S20144144/article/details/103417502

SGD原理



### 学习率scheduler
- https://www.kaggle.com/code/isbhargav/guide-to-pytorch-learning-rate-scheduling


## 损失函数
- https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-pytorch-loss-functions.md

cross entropy 
- https://gombru.github.io/2018/05/23/cross_entropy_loss/
binary cross entropy


对数损失
- https://www.zhihu.com/question/27126057


## 网络

### word2vec/glove/fasttext

### RNN/LSTM/GRU
公式

梯度爆炸与梯度消失

### Transformer

- attention

### BERT

### GPT

### YOLO

### wide&deep
