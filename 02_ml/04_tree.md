# 决策树

## 分裂依据

### 熵
entropy公式：

- 信息增益

### 基尼系数

### squared loss


## adaboost
- 损失函数exp
- 对分类正确的样本降低权重，对错误分类的样本升高或者保持全中不变。在模型融合过程中，根据错误率对基分类器器进行加权融合，错误率低的分类器拥有更大的“话语权”


## 随机森林

- 损失函数：

- 可以并行训练，不容易过拟合

## 梯度提升树GBDT

- GBDT拟合的是负梯度。当损失函数为平方损失的时候，负梯度正好为残差
- 做分类任务时，GBDT内部每棵树是回归树，不论是回归还是分类任务


## xgboost
- XGBoost使用二阶泰勒展开表示梯度，即每棵树拟合的是二阶泰勒的梯度，相比GBDT的一阶泰勒展开、对梯度的表示更准确
- 损失函数中显式加入了正则项，对叶子数目和叶子权重做惩罚


### 问题
- 如何并行

    - Boosting算法的弱学习器是没法并行迭代的，但是单个弱学习器里面最耗时的是决策树的分裂过程，XGBoost针对这个分裂做了比较大的并行优化。在训练之前，预先对每个特征内部进行了排序找出候选切割点，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算可以多线程进行，即在不同的特征属性上采用多线程并行方式寻找最佳分割点。

- 如何快速分裂


- 分类节点的依据


- 如何处理缺失值

    - 寻找split point的时候，不会对该特征为missing的样本进行遍历统计，只对该列特征值为non-missing的样本对应的特征值进行遍历，通过这个工程技巧来减少了为稀疏离散特征寻找split point的时间开销
    - 为缺失值或者指定的值指定分支的默认方向，为了保证完备性，分别处理将missing该特征值的样本分配到左叶子结点和右叶子结点的两种情形，分到那个子节点带来的增益大，默认的方向就是哪个子节点，这能大大提升算法的效率
    - 推理时，如果训练过程中，出现过缺失值，则按照训练过程中缺失值划分的方向（left or right），进行划分；如果训练过程中，没有出现过缺失值，将缺失值的划分到默认方向（左子树）

- 如何正则化的
  - 代价函数里加入了正则项，树的叶子节点个数、每个叶子节点上输出的score的L2

- 树模型的缺点
  - 在回归的外推问题

- 模型细节
  - 模型里有特别大的值对结果有什么影响，需要如何处理

## lightgbm
- lightgbm遍历每个特征寻找最优分裂点时，将每个特征进行了分桶，比如可指定分为64个桶，那么该特征所有的值都落入这64个桶中，遍历这个特征时，最多只需要遍历64次，则每次分裂的复杂度为O(特征数*桶数)，如果不分桶，则可能所有样本的值都不同，则复杂度为O(特征数*样本数)。
为什么能分桶：因为每棵树都是弱分类器，不需要非常精准，且分桶一定程度上提高了泛化能力，降低了误差
- lightgbm的分枝模式为leaf-wise，即遍历当前所有待分枝节点，不需要一定在最下边一层，谁的分裂增益大就分谁。而XGBoost的分枝模式为level-wise，即分完一层再分下一层，可能一层中有些叶子分裂增益极小，但是仍然要花费时间和空间去分裂

## catboost

## 特征重要性
- 所有树中作为划分属性的次数
- 使用特征在作为划分属性时loss平均的降低量
- 作为划分属性时对样本的覆盖度

- permutation
- SHAP

## 代码

Decision Tree


```
import numpy as np

class Decision_node():
    pass

class Decision_tree():
    pass
```


## 问题
- xgboost的cache awareness如何提高计算效率？
  - 


## 参考
- [从sklearn源码简析GBDT](https://mp.weixin.qq.com/s/iKxv9-fHJp2DFQyeWlvTgQ)
